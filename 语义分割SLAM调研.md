现有的建图方式会有大量特征点选取在路边停靠的车辆上，这会干扰后续定位的精度，一个解决思路是用现有的语义分割/目标检测网络，去除车辆、行人上的特征点，避免动态物体、临时停靠物体在SLAM过程中的干扰。

## MaskSLAM
删除车以及天空的特征点，但用但模拟器生成数据，且语义分割部分比较局限。

## DS-SLAM
用SegNet做语义分割，生成的是稠密语义地图。和我们的目标有一定差别。

## DynaSLAM
目前的可应用于动态环境中的SLAM方法大多无法应对下面两种情况：
1. 当一个先验的动态对象保持静态时，例如，停放的汽车或坐着的人
2. 静态对象产生的变化，比如一个人推的椅子

对于SLAM问题，提出了以下疑问：
1. 如何在图像中检测到上述这样的动态对象。
2. 防止跟踪算法使用属于动态对象部分进行匹配。
3. 防止映射算法将移动对象作为3D地图的一部分。

DynaSLAM 提出了一种在线算法来处理RGB-D、双目和单目SLAM中的动态对象。这是通过在ORB-SLAM2系统添加一个前端来实现的，其目的是获得更精确的跟踪和场景的可重用映射。
在单目和双目的情况下，使用Mask R-CNN 对帧内的先验动态对象(如人和车)进行像素级分割，这样SLAM算法就不会提取这些对象的特征。直接将图像传入Mask R-CNN 中进行分割，将具有先验动态信息的物体分割出去，仅使用剩下的图像进行跟踪和建图处理。

Mask R-CNN 的输入是RGB原始图像。目的是划分那些潜在动态或可移动的类（人，自行车，汽车，摩托车，飞机，公共汽车，火车，卡车，船，鸟，猫，狗，马，羊，牛，大象，熊，斑马和长颈鹿）。对于大多数环境，可能出现的动态对象都包含在此列表中。

假设输入是大小为m×n×3的RGB图像，网络的输出是大小为m×n×l的矩阵，其中l是图像中的对象(类别)数，再将l层分类图像合并成一幅图像。

该文章还提出了基于 Mask R-CNN 和多视几何的动态物体分割，主要是针对性地处理在 Mask RCNN 中没有先验动态标记而具有移动性的物体的分割，例如行人手中的书等。在此不详细展开。

实验结果：当不在具有先验动态信息的物体上提取特征时，跟踪精度会比较高，诸如汽车，自行车等；当在大多数车辆都是停止状态时，DynaSLAM 的误差会大一些，因为将这些物体对象去掉之后检测到的特征点会处于较远或纹理较弱的区域，从而导致跟踪精度下降。但是在回环检测和重定位处理中结果较为稳健，因为构建的地图中仅仅包含结构性物体对象，没有动态物体。

缺点：作者提到所做的实验中并没有对算法进行工程优化，因此时间上的开销会比较大。

## 总结与一些想法
1. 有较多的研究放在语义与 ORB-SLAM2 结合的项目，用 Mask R-CNN 或者 SegNet 去除具有先验动态信息的物体的特征点，不参与建图与跟踪。
2. 第一个难点是，如何在C++中调用训练好的神经网络。查了一下，对OpenCV等库的版本有较高要求，但是ORB-SLAM3对这些库对版本也有要求，到时候有冲突的话估计debug比较艰难。
3. 另一大难点在于，实时性较差，查到的资料称C++版的 MaskRCNN 的每帧推断时间大约几百到2000毫秒，在定位环节是否还有调用语义分割的必要？能否仅在建图环节用语义分割。
4. 能不能通过ROS通信，将掩膜的步骤交给python环境去做，然后将掩膜结果发布ROS话题让ORB3订阅。这样在技术上实现似乎更简单。亦或者是说先预处理建图数据，将其中的干扰项删掉，再用生成的素材去建图？
